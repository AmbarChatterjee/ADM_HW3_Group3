{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "##1. Data collection\n",
        "For this homework, there is no provided dataset. Instead, you have to build your own. Your search engine will run on text documents. So, here we detail the procedure to follow for the data collection. We strongly suggest you work on different modules when implementing the required functions. For example, you may have a crawler.py module, a parser.py module, and a engine.py module: this is a good practice that improves readability in reporting and efficiency in deploying the code. Be careful; you are likely dealing with exceptions and other possible issues!"
      ],
      "metadata": {
        "id": "KGkhoku_QJtZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###1.1. Get the list of master's degree courses\n",
        "We start with the list of courses to include in your corpus of documents. In particular, we focus on web scrapping the MSc Degrees. Next, we want you to collect the URL associated with each site in the list from the previously collected list. The list is long and split into many pages. Therefore, we ask you to retrieve only the URLs of the places listed in the first 400 pages (each page has 15 courses, so you will end up with 6000 unique master's degree URLs).\n",
        "\n",
        "The output of this step is a .txt file whose single line corresponds to the master's URL."
      ],
      "metadata": {
        "id": "bHk9J6jDQP4Z"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "miz0xb3JOlvf"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import requests\n",
        "from datetime import datetime\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "from  tqdm import tqdm\n",
        "import time\n",
        "import re\n",
        "import csv\n",
        "from crawler import scrape_urls\n",
        "from crawler import scrape_htmls\n",
        "from parser import extract_course_info"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#specify the URL of the degree page\n",
        "url = 'https://www.findamasters.com/masters-degrees/msc-degrees'\n",
        "#specify the total number of pages to scrape\n",
        "pages = 400\n",
        "#specify the output file name for URLs\n",
        "urlfilename = 'urls.txt'\n",
        "#now we call the function to scrape urls from our crawler.py script\n",
        "scrape_urls(url, pages, urlfilename)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v8s81UWVgxED",
        "outputId": "14d67b32-9c99-4537-e25c-b9a3b97ec56d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 400/400 [09:56<00:00,  1.49s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "The URLs are now saved in the file urls.txt\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "*We used beautiful soup to do the webscraping and added a delay time to prevent our IP from being blocked by the website. After the scraping, we saved the urls in a urls.txt file so that we just need to do this process once in the beginning of the project. As there are 400 pages with 15 master degree urls each, our txt file got 6000 unique urls to deal with.*"
      ],
      "metadata": {
        "id": "KIbuBOQHPSrF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###1.2. Crawl master's degree pages\n",
        "Once you get all the URLs in the first 400 pages of the list, you:\n",
        "\n",
        "Download the HTML corresponding to each of the collected URLs.\n",
        "After you collect a single page, immediately save its HTML in a file. In this way, if your program stops for any reason, you will not lose the data collected up to the stopping point.\n",
        "Organize the downloaded HTML pages into folders. Each folder will contain the HTML of the courses on page 1, page 2, ... of the list of master's programs."
      ],
      "metadata": {
        "id": "H4oZTfL8gpXc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#specify the url file name\n",
        "urlfilename = 'urls.txt'\n",
        "#specify the folder to contain the HTML files\n",
        "htmlfolderName= 'newFolderHTML'\n",
        "#now we call the function to scrape the HTMLs from the crawler script\n",
        "scrape_htmls(urlfilename,htmlfolderName)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5bx6bCD8KLcI",
        "outputId": "ab3766aa-d8b3-4d75-eec7-5fdf24bb9213"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 6000/6000 [2:18:18<00:00,  1.38s/it]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we are crawling over the master degree course pages from the urls.txt file we already have in our directory. We created an output_folder by the name 'newFolderHTML' where we have stored each of the HTMl files we get from the 6000 URLs with the name- Html-1.txt, Html-2.txt unitl ...Html-6000.txt. We will use this folder of HTMls to do the next question..."
      ],
      "metadata": {
        "id": "J7taQQAEg6d8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###1.3 Parse downloaded pages\n",
        "At this point, you should have all the HTML documents about the master's degree of interest, and you can start to extract specific information. The list of the information we desire for each course and their format is as follows:\n",
        "\n",
        "Course Name (to save as courseName): string;\n",
        "University (to save as universityName): string;\n",
        "Faculty (to save as facultyName): string\n",
        "Full or Part Time (to save as isItFullTime): string;\n",
        "Short Description (to save as description): string;\n",
        "Start Date (to save as startDate): string;\n",
        "Fees (to save as fees): string;\n",
        "Modality (to save as modality):string;\n",
        "Duration (to save as duration):string;\n",
        "City (to save as city): string;\n",
        "Country (to save as country): string;\n",
        "Presence or online modality (to save as administration): string;\n",
        "Link to the page (to save as url): string.\n",
        "\n",
        "For each master's degree, you create a course_i.tsv file of this structure:\n",
        "\n",
        "courseName \\t universityName \\t  ... \\t url"
      ],
      "metadata": {
        "id": "og3VlYEZiJoo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*To parse all the 6000 HTML documents from the HTML folder, we define a generic function that will scoop out the required specific information (if present) from the corresponding HTML files and then we call the function with the html folder name and the tsv output folder name*\n",
        "\n",
        "*Our function takes three parameters: the html folder name, folder name to put the .tsv files inside and the urls.txt file that has the 6000 urls. Based on those 60000 urls, the function fetches the Html-i.txt file from the html folder corresponding to ith Url of the Urls.txt and scoops the information of course, university, faculty, etc using beautiful soup. It creates two rows, one as the header rows with the column names and another has the data rows with the column data and creates a course-i.tsv file with that in the tsv output folder.*"
      ],
      "metadata": {
        "id": "jYYwaZDUiUXj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Specify the folder containing the HTML files and the output folder for TSV files and the urls.txt file\n",
        "html_folder = 'newFolderHTML'\n",
        "output_folder = 'folderTSV'\n",
        "urls_file = 'urls.txt'\n",
        "# now we call the function from the parser.py script to extract course information\n",
        "extract_course_info(html_folder, output_folder, urls_file)"
      ],
      "metadata": {
        "id": "pcNthYoSkfRc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "*after this, all the necessary course_1.tsv, course_2.tsv,....,course_6000.tsv are now in the 'folderTSV'*"
      ],
      "metadata": {
        "id": "4V_f0L8dlHNE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is the end of the data collection process. We have scraped the 400 pages of the degree pages website to get 6000 urls and crawled over those urls to download the HTML of each and then using beautiful soup we extracted all the information we are interested in from the 6000 urls and created 6000 Tab Separated Value (.tsv) files inside another folder. The most time consuming of all was the downloading of the HTML files looking at each of the 6000 urls. With a waiting time of 1 second, it took around 1.5 to 2 hours and with a waiting time of 2 seconds, it took double the time. The problem is, if we don't put a wait time of 2 seconds, some of the urls are not loading within that 1 second threshold, because of too many requests being provided, the website is blocking the IP address. This initially resulted in blank HTML files being downloaded for those few URLS. But with a waiting time of 2 seconds, the problem could be averted, and each of the 6000 HTML was correct. Only that we had to fragment the range of the urls.txt list we were using. Initially ran, with a range 1 to 1000, then reran with 10001 to 3000 and a last run with 30001 to 6000, actually averted the issue. To speed up the process, I employed multi-threading, but it scraped the 6000 pages in 1 minute and only downloaded 100 HTMl items with maximum 50 threads. As the load is quite high, 6000, the threads needs to be higher, but it means a considerable load on the server resulting in IP blocking again. Hence, I had to drop that idea and do it the normal way, trading off time consumption with accuracy. It took 4 hours to run, but at the end, I got all the 6000 HTMLs correctly.\n",
        "\n",
        "My conclusion is: the webscraping is not so optimised with the beautiful soup framework. Perhaps, we can have a look at an alternative framework that is more optimised and meant for this purpose. For example: scrapy."
      ],
      "metadata": {
        "id": "uAX03LZvrxsx"
      }
    }
  ]
}