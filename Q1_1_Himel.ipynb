{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "##1. Data collection\n",
        "For this homework, there is no provided dataset. Instead, you have to build your own. Your search engine will run on text documents. So, here we detail the procedure to follow for the data collection. We strongly suggest you work on different modules when implementing the required functions. For example, you may have a crawler.py module, a parser.py module, and a engine.py module: this is a good practice that improves readability in reporting and efficiency in deploying the code. Be careful; you are likely dealing with exceptions and other possible issues!"
      ],
      "metadata": {
        "id": "KGkhoku_QJtZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###1.1. Get the list of master's degree courses\n",
        "We start with the list of courses to include in your corpus of documents. In particular, we focus on web scrapping the MSc Degrees. Next, we want you to collect the URL associated with each site in the list from the previously collected list. The list is long and split into many pages. Therefore, we ask you to retrieve only the URLs of the places listed in the first 400 pages (each page has 15 courses, so you will end up with 6000 unique master's degree URLs).\n",
        "\n",
        "The output of this step is a .txt file whose single line corresponds to the master's URL."
      ],
      "metadata": {
        "id": "bHk9J6jDQP4Z"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "miz0xb3JOlvf"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "from datetime import datetime\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "from  tqdm import tqdm\n",
        "import time\n",
        "import re\n",
        "import csv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hgmAbNbZM0Ns",
        "outputId": "c17ffff5-19e9-4e95-8dd1-2179359ed6d0"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 400/400 [19:45<00:00,  2.96s/it]\n"
          ]
        }
      ],
      "source": [
        "master_urls = [] #we create an empty list to which we will append the urls we get\n",
        "for i in tqdm(range(1,401)):\n",
        "\n",
        "    #We change along the first 400 pages and request via their urls\n",
        "    url='https://www.findamasters.com/masters-degrees/msc-degrees?page='+str(i)\n",
        "    result=requests.get(url)\n",
        "    soup=BeautifulSoup(result.text,'html.parser')\n",
        "\n",
        "    try:\n",
        "         # Find and extract the URLs of master's degree courses\n",
        "        course_links = soup.find_all('a', class_='courseLink text-dark')\n",
        "        for link in course_links:\n",
        "            course_url = link.get('href')\n",
        "            master_urls.append('https://www.findamasters.com'+course_url)\n",
        "\n",
        "        #To conclude, we save it in a .txt file with all the urls of the courses\n",
        "        with open('urls.txt', 'w') as file:\n",
        "          for url in master_urls:\n",
        "            file.write(url + '\\n')\n",
        "    except:\n",
        "        pass\n",
        "    # This will leave a second between each iteration so we do not get banned from the website\n",
        "    time.sleep(1)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "*As you can see, we used beautiful soup to do the webscraping and added a delay time to prevent our IP from being blocked by the website. After the scraping, we saved the urls in a urls.txt file so that we just need to do this process once in the beginning of the project. As there are 400 pages with 15 master degree urls each, our txt file got 6000 unique urls to deal with.*"
      ],
      "metadata": {
        "id": "KIbuBOQHPSrF"
      }
    }
  ]
}