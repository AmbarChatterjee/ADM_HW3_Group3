{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Define a new score!\n",
    "\n",
    "Now it's our turn: build a new metric to rank MSc degrees.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have put our custom functions in functions.py and the search engine functions for this answer in searchEngineNew.py and the search engine that we recall from the question 2 in searchEngine.py and then we put all the library that are useful to run our code and we are importing them in the beginning here so to run the code please before run this following piece of code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import pandas as pd\n",
    "import json\n",
    "from searchEngine import conjunction_search\n",
    "from searchEngineNew import calculate_score\n",
    "from searchEngineNew import top_k_documents\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before going on we have to import our data so we can do our task, it is crucial! We store our data in a .tsv file so we don't have to pre-process always the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "courses_df=pd.read_csv('courses_data_processed.tsv',sep='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the date we are importing the vocabulary and the first inverted index that we created (we saved both previously so that we have to not run every time the code to have those). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Load vocabulary from \"vocabulary.txt\"\n",
    "vocabulary = {}\n",
    "with open(\"vocabulary.txt\", \"r\") as vocab_file:\n",
    "    for line in vocab_file:\n",
    "        term, term_id = line.strip().split()\n",
    "        vocabulary[term] = int(term_id)\n",
    "\n",
    "# Load inverted index from \"inverted_index.json\"\n",
    "with open(\"inverted_index.json\", \"r\") as index_file:\n",
    "    inverted_index = json.load(index_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Now we arrive to the core of the task: define a new score and sort the query-related documents according to this new score.**\n",
    "\n",
    "- **New score used:** we decided to use a weighted combination of various fields in our dataset, we thought that not only the 'description' field is relevant for the query because an user might want to search for master's degrees based on a city or based on fees payable so each field contributes to the score based on its weight, and the overall score for a document is the sum of the normalized values of numeric fields and binary scores for string fields, so the weights reflect the importance of each field in the scoring process.\n",
    "\n",
    "- **Structure to solve the task:** After defining a new score, we sorted the documents according to it, to do this we use the heap data structure that keeps us top-k documents. \n",
    "\n",
    "We decided we wanted to have the top-10 documents that best matched the user's query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query given in input from a user\n",
    "query = input(\"Enter your query: \")\n",
    "\n",
    "# Columns of our dataser that we want to use for the score\n",
    "selected_columns = ['courseName', 'universityName', 'facultyName', 'isItFullTime',\n",
    "       'description', 'startDate', 'fees', 'modality', 'duration', 'city',\n",
    "       'country', 'administration', 'url', 'ProcessedDescription', 'currency',\n",
    "       'fees (EUR)']\n",
    "\n",
    "# Usage of the conjunction_search done in 2.1 to retrieve documents that matches with the query\n",
    "result_df = conjunction_search(courses_df, vocabulary, inverted_index, query)\n",
    "\n",
    "# Set the columns that we are using for scoring\n",
    "documents = result_df[selected_columns].to_dict(orient='records')\n",
    "\n",
    "# Define weights for scoring that reflect the importance of each field in the scoring process\n",
    "weights = {\n",
    "    \"courseName\": 0.1,\n",
    "    \"universityName\": 0.2,\n",
    "    \"facultyName\": 0.15,\n",
    "    \"isItFullTime\": 0.05,\n",
    "    \"description\": 0.2,\n",
    "    \"startDate\": 0.08,\n",
    "    \"fees\": 0.05,\n",
    "    \"modality\": 0.1,\n",
    "    \"duration\": 0.1,\n",
    "    \"city\": 0.1,\n",
    "    \"country\": 0.2,\n",
    "    \"administration\": 0.15,\n",
    "    \"url\": 0.05,\n",
    "    'ProcessedDescription':0.1, \n",
    "    'currency':0.05,\n",
    "    'fees (EUR)':0.05\n",
    "}\n",
    "\n",
    "# Number of top-k documents you want to retrieve\n",
    "k = 10\n",
    "\n",
    "# Retrieve and print the top-k documents using the function in searchEngineNew.py\n",
    "result = top_k_documents(documents, query, weights, k)\n",
    "result[['courseName','universityName','description','url', 'MyScore']]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Let's now focus on analyzing the results obtained with our new score function and the function based on cosine similarity (done in the question 2.2)**\n",
    "\n",
    "In question 2.2 we used cosine similarity to sort documents based on their tf-idf, at first glance reflecting without comparing the results it might seem that just that function works better than our new score because it is based on mathematical computations and in particular it is based on tf-idf which is what is used purely in text mining. \n",
    "Comparing the results, however, we can make the following observations:\n",
    "- The function based on cosine similarity exploits and processes only the 'description' column of our dataset thus going to extract query-related documents based on the 'description' field\n",
    "- The function based on our new score processes all columns in our dataset and then goes to extract query-related documents by scrolling and 'weighing' each field in our dataset\n",
    "This leads us to conclude that in case a user enters a query in which there is the name of a specific country/university or a range of fees for his master degree program the function we described in 2.2 is definitely less effective than the new score we created because the latter goes and parses all the fields in the dataset.\n",
    "\n",
    "On the other hand if we use as a score the cosine similarity being a well-defined metric in the case where the query is the same when we go to apply the two score functions then in that case the precision is greater.\n",
    "So we can conclude that based on our dataset and how we have processed the columns and data what is the best score function depends on what we are interested in and then it depends on the particular query.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
